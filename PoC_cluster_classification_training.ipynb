{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V100","mount_file_id":"1UKHHfWEDVNANsGjhjk6TNLECQrYOSim_","authorship_tag":"ABX9TyNs9z7q8o8e0dk24hZ7ZwW9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d1NAHNywqTe4","executionInfo":{"status":"ok","timestamp":1701727570726,"user_tz":-120,"elapsed":6891,"user":{"displayName":"Arie Genkin","userId":"17117459664307926572"}},"outputId":"0bf684d9-c5ac-453b-c8ae-bf306942f1e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q datasets"]},{"cell_type":"code","source":["import numpy as np\n","import pickle\n","import keras\n","from keras import layers\n","from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback\n","from keras.applications.vgg19 import VGG19\n","\n","folder = \"/content/drive/MyDrive/Colab Notebooks/Zama\"\n","num_classes = 80\n","batch_size = 16\n","\n","# load embedding model\n","siamese_embedding_model = keras.models.load_model(f'{folder}/embedding_network_512.h5')     #  USAGE: res = siamese_embedding_model.predict( tf_ds )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AzTiFHHtzsw4","executionInfo":{"status":"ok","timestamp":1701730054977,"user_tz":-120,"elapsed":17822,"user":{"displayName":"Arie Genkin","userId":"17117459664307926572"}},"outputId":"029833b8-42dd-4cd0-f905-166494c41e00"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PbPa-WxI8EaG"},"outputs":[],"source":["from datasets import load_dataset, DatasetDict\n","\n","def custom_train_test_split( spec ):\n","\n","    # Split into:\n","    # train      90%\n","    # validation  5%\n","    # test        5%\n","    # following https://stackoverflow.com/questions/76001128/splitting-dataset-into-train-test-and-validation-using-huggingface-datasets-fun\n","    spec = spec.train_test_split(test_size=0.1)\n","\n","    data_train = spec['train']\n","    data_val_test = spec['test']\n","\n","    data_val_test = data_val_test.train_test_split(test_size=0.5)\n","\n","    return (data_train, data_val_test['train'], data_val_test['test'] )"]},{"cell_type":"markdown","metadata":{"id":"yUUgBtLk2yCz"},"source":["#### LIbraries for model creation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FioFYgJDySQb"},"outputs":[],"source":["# image dimensions\n","rows = 224\n","cols = 224\n","\n","def create_cls_model_fn( num_classes):\n","    # create a vgg19 model and discard the top layer\n","    model_vgg19 = VGG19(include_top=False, weights='imagenet', input_shape=(rows, cols, 3))\n","    model_vgg19.trainable = False\n","\n","    image = keras.Input(shape=(224, 298, 3))\n","    embedding = keras.Input(shape=(512))\n","\n","    x = layers.RandomCrop(rows, cols)(image)\n","    x = layers.Rescaling(scale=1.0 / 127.5, offset=-1)(x)\n","\n","    x = model_vgg19(x)\n","\n","    x = layers.Flatten()(x)\n","    x = layers.Dense(2048, activation='relu')(x)\n","    x = layers.Dropout(0.2)(x)\n","    x = layers.Dense(2048, activation='relu')(x)\n","\n","    outputs = keras.layers.Concatenate()([x, embedding])\n","    outputs = layers.Dense(num_classes, activation='softmax')(outputs)\n","\n","    model = keras.Model(inputs=[image, embedding], outputs=outputs, name='cluster_classification')\n","\n","    # Compile model\n","    model.compile(\n","        optimizer= keras.optimizers.SGD(learning_rate=0.005),\n","        loss= keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n","\n","        metrics = [keras.metrics.SparseCategoricalAccuracy()]\n","    )\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"T9Lf4UikN8tH"},"source":["### Cluster classification"]},{"cell_type":"code","source":["# A spectrogram images dataset for classification of tracks per cluster\n","cluster_cls_ds_handle = '8000_large_4_siamclusters'    #'siamese_clusters'                                #  '8000_large_4_siamclusters'\n","spec = load_dataset(f\"arieg/{cluster_cls_ds_handle}\")\n","spec.set_format('tf')\n","\n","# Split into train / eval / test\n","ds_train, ds_eval, ds_test = custom_train_test_split(spec['train'])\n","\n","# Train\n","image_train = ds_train['image'].numpy()\n","emb_train = siamese_embedding_model.predict( ds_train.with_format('tf').to_tf_dataset(columns=\"image\", batch_size=batch_size))\n","label_train = np.array( ds_train['siam_cluster'] ).astype(\"float32\")\n","print(image_train.shape, emb_train.shape, label_train.shape)\n","\n","# Eval\n","image_eval = ds_eval['image'].numpy()\n","emb_eval = siamese_embedding_model.predict( ds_eval.with_format('tf').to_tf_dataset(columns=\"image\", batch_size=batch_size))\n","label_eval = np.array( ds_eval['siam_cluster'] ).astype(\"float32\")\n","print(image_eval.shape, emb_eval.shape, label_eval.shape)\n","\n","# Test\n","image_test = ds_test['image'].numpy()\n","emb_test = siamese_embedding_model.predict( ds_test.with_format('tf').to_tf_dataset(columns=\"image\", batch_size=batch_size))\n","label_test = np.array( ds_test['siam_cluster'] ).astype(\"float32\")\n","print(image_test.shape, emb_test.shape, label_test.shape)\n","\n","# Make a classification model for this cluster\n","model = create_cls_model_fn( num_classes )\n","\n","# Training\n","epochs = 20\n","history = model.fit([image_train, emb_train], label_train, epochs=epochs, validation_data=([image_eval, emb_eval], label_eval), callbacks=[], verbose=1)\n","\n","# save training history and model\n","with open(f'{folder}/8000_large_4_siamclusters_enhanced.history', 'wb') as file_pi:\n","    pickle.dump(history.history, file_pi)\n","\n","model.save(f'{folder}/8000_large_4_siamclusters_enhanced.keras')"],"metadata":{"id":"6qyHEGNPdo6v"},"execution_count":null,"outputs":[]}]}