{"cells":[{"cell_type":"markdown","source":["### Adopted from https://github.com/zama-ai/concrete-ml/tree/main/use_case_examples/cifar/cifar_brevitas_finetuning with modifications"],"metadata":{"id":"N1hl0uFFJJd6"}},{"cell_type":"markdown","source":["### Requirements"],"metadata":{"id":"Y2tjZNi5eDjg"}},{"cell_type":"code","source":["!pip install  concrete-ml brevitas"],"metadata":{"id":"LN3BlmWMuUCN","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1702760333241,"user_tz":-120,"elapsed":151949,"user":{"displayName":"Arie Genkin","userId":"17117459664307926572"}},"outputId":"16a88842-e50a-441f-dfdf-c3d1b0547da6"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting concrete-ml\n","  Downloading concrete_ml-1.3.0-py3-none-any.whl (202 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.5/202.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting brevitas\n","  Downloading brevitas-0.10.0-py3-none-any.whl (600 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m600.8/600.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting boto3<2.0.0,>=1.23.5 (from concrete-ml)\n","  Downloading boto3-1.34.2-py3-none-any.whl (139 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting brevitas\n","  Downloading brevitas-0.8.0-py3-none-any.whl (357 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.3/357.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting certifi==2023.07.22 (from concrete-ml)\n","  Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.3/158.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting concrete-python==2.5.0-rc1 (from concrete-ml)\n","  Downloading concrete_python-2.5.0rc1-cp310-cp310-manylinux_2_28_x86_64.whl (64.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fastapi<0.103.0,>=0.102.0 (from concrete-ml)\n","  Downloading fastapi-0.102.0-py3-none-any.whl (66 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gitpython==3.1.35 (from concrete-ml)\n","  Downloading GitPython-3.1.35-py3-none-any.whl (188 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.8/188.8 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting hummingbird-ml[onnx]==0.4.8 (from concrete-ml)\n","  Downloading hummingbird_ml-0.4.8-py2.py3-none-any.whl (164 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.6/164.6 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.10/dist-packages (from concrete-ml) (1.23.5)\n","Collecting onnx==1.13.1 (from concrete-ml)\n","  Downloading onnx-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting onnxoptimizer==0.3.10 (from concrete-ml)\n","  Downloading onnxoptimizer-0.3.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (671 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m671.6/671.6 kB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting onnxruntime==1.13.1 (from concrete-ml)\n","  Downloading onnxruntime-1.13.1-cp310-cp310-manylinux_2_27_x86_64.whl (4.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: protobuf==3.20.3 in /usr/local/lib/python3.10/dist-packages (from concrete-ml) (3.20.3)\n","Collecting pytest-json-report==1.5.0 (from concrete-ml)\n","  Downloading pytest_json_report-1.5.0-py3-none-any.whl (13 kB)\n","Collecting scikit-learn==1.1.3 (from concrete-ml)\n","  Downloading scikit_learn-1.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting scipy==1.10.1 (from concrete-ml)\n","  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting setuptools==65.6.3 (from concrete-ml)\n","  Downloading setuptools-65.6.3-py3-none-any.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting skops==0.5.0 (from concrete-ml)\n","  Downloading skops-0.5.0-py3-none-any.whl (92 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.8/92.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting skorch==0.11.0 (from concrete-ml)\n","  Downloading skorch-0.11.0-py3-none-any.whl (155 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.1/155.1 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch==1.13.1 (from concrete-ml)\n","  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from concrete-ml) (4.66.1)\n","Requirement already satisfied: transformers<5.0.0,>=4.30.2 in /usr/local/lib/python3.10/dist-packages (from concrete-ml) (4.35.2)\n","Requirement already satisfied: typing-extensions==4.5.0 in /usr/local/lib/python3.10/dist-packages (from concrete-ml) (4.5.0)\n","Collecting uvicorn<0.22.0,>=0.21.0 (from concrete-ml)\n","  Downloading uvicorn-0.21.1-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting xgboost==1.6.2 (from concrete-ml)\n","  Downloading xgboost-1.6.2-py3-none-manylinux2014_x86_64.whl (255.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.9/255.9 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from brevitas) (23.2)\n","Collecting dependencies==2.0.1 (from brevitas)\n","  Downloading dependencies-2.0.1-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: networkx>=2.6 in /usr/local/lib/python3.10/dist-packages (from concrete-python==2.5.0-rc1->concrete-ml) (3.2.1)\n","Collecting z3-solver>=4.12 (from concrete-python==2.5.0-rc1->concrete-ml)\n","  Downloading z3_solver-4.12.4.0-py2.py3-none-manylinux2014_x86_64.whl (56.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gitdb<5,>=4.0.1 (from gitpython==3.1.35->concrete-ml)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting dill (from hummingbird-ml[onnx]==0.4.8->concrete-ml)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting onnxconverter-common>=1.6.0 (from hummingbird-ml[onnx]==0.4.8->concrete-ml)\n","  Downloading onnxconverter_common-1.14.0-py2.py3-none-any.whl (84 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from hummingbird-ml[onnx]==0.4.8->concrete-ml) (5.9.5)\n","Collecting onnxmltools<=1.11.0,>=1.6.0 (from hummingbird-ml[onnx]==0.4.8->concrete-ml)\n","  Downloading onnxmltools-1.11.0-py2.py3-none-any.whl (302 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting skl2onnx<=1.12.0,>=1.7.0 (from hummingbird-ml[onnx]==0.4.8->concrete-ml)\n","  Downloading skl2onnx-1.12-py2.py3-none-any.whl (279 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.3/279.3 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting coloredlogs (from onnxruntime==1.13.1->concrete-ml)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.13.1->concrete-ml) (23.5.26)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.13.1->concrete-ml) (1.12)\n","Requirement already satisfied: pytest>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from pytest-json-report==1.5.0->concrete-ml) (7.4.3)\n","Collecting pytest-metadata (from pytest-json-report==1.5.0->concrete-ml)\n","  Downloading pytest_metadata-3.0.0-py3-none-any.whl (10 kB)\n","Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3->concrete-ml) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3->concrete-ml) (3.2.0)\n","Requirement already satisfied: huggingface-hub>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from skops==0.5.0->concrete-ml) (0.19.4)\n","Requirement already satisfied: tabulate>=0.8.8 in /usr/local/lib/python3.10/dist-packages (from skops==0.5.0->concrete-ml) (0.9.0)\n","Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.1->concrete-ml)\n","  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.1->concrete-ml)\n","  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.1->concrete-ml)\n","  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.1->concrete-ml)\n","  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->concrete-ml) (0.42.0)\n","Collecting botocore<1.35.0,>=1.34.2 (from boto3<2.0.0,>=1.23.5->concrete-ml)\n","  Downloading botocore-1.34.2-py3-none-any.whl (11.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m111.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.23.5->concrete-ml)\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.10.0,>=0.9.0 (from boto3<2.0.0,>=1.23.5->concrete-ml)\n","  Downloading s3transfer-0.9.0-py3-none-any.whl (82 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi<0.103.0,>=0.102.0->concrete-ml) (1.10.13)\n","Collecting starlette<0.28.0,>=0.27.0 (from fastapi<0.103.0,>=0.102.0->concrete-ml)\n","  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.30.2->concrete-ml) (3.13.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.30.2->concrete-ml) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.30.2->concrete-ml) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.30.2->concrete-ml) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.30.2->concrete-ml) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.30.2->concrete-ml) (0.4.1)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn<0.22.0,>=0.21.0->concrete-ml) (8.1.7)\n","Collecting h11>=0.8 (from uvicorn<0.22.0,>=0.21.0->concrete-ml)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.2->boto3<2.0.0,>=1.23.5->concrete-ml) (2.8.2)\n","Requirement already satisfied: urllib3<2.1,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.2->boto3<2.0.0,>=1.23.5->concrete-ml) (2.0.7)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython==3.1.35->concrete-ml)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.1->skops==0.5.0->concrete-ml) (2023.6.0)\n","INFO: pip is looking at multiple versions of onnxconverter-common to determine which version is compatible with other requirements. This could take a while.\n","Collecting onnxconverter-common>=1.6.0 (from hummingbird-ml[onnx]==0.4.8->concrete-ml)\n","  Downloading onnxconverter_common-1.13.0-py2.py3-none-any.whl (83 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.8/83.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=3.8.0->pytest-json-report==1.5.0->concrete-ml) (2.0.0)\n","Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.8.0->pytest-json-report==1.5.0->concrete-ml) (1.3.0)\n","Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.8.0->pytest-json-report==1.5.0->concrete-ml) (1.2.0)\n","Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.8.0->pytest-json-report==1.5.0->concrete-ml) (2.0.1)\n","Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.103.0,>=0.102.0->concrete-ml) (3.7.1)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime==1.13.1->concrete-ml)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.30.2->concrete-ml) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.30.2->concrete-ml) (3.6)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime==1.13.1->concrete-ml) (1.3.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.103.0,>=0.102.0->concrete-ml) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.2->boto3<2.0.0,>=1.23.5->concrete-ml) (1.16.0)\n","Installing collected packages: z3-solver, smmap, setuptools, scipy, onnx, nvidia-cuda-nvrtc-cu11, jmespath, humanfriendly, h11, dill, dependencies, certifi, xgboost, uvicorn, starlette, scikit-learn, pytest-metadata, onnxoptimizer, onnxconverter-common, nvidia-cuda-runtime-cu11, nvidia-cublas-cu11, gitdb, coloredlogs, botocore, skorch, skl2onnx, s3transfer, pytest-json-report, onnxruntime, nvidia-cudnn-cu11, gitpython, fastapi, torch, skops, onnxmltools, boto3, hummingbird-ml, concrete-python, brevitas, concrete-ml\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 67.7.2\n","    Uninstalling setuptools-67.7.2:\n","      Successfully uninstalled setuptools-67.7.2\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.11.4\n","    Uninstalling scipy-1.11.4:\n","      Successfully uninstalled scipy-1.11.4\n","  Attempting uninstall: certifi\n","    Found existing installation: certifi 2023.11.17\n","    Uninstalling certifi-2023.11.17:\n","      Successfully uninstalled certifi-2023.11.17\n","  Attempting uninstall: xgboost\n","    Found existing installation: xgboost 2.0.2\n","    Uninstalling xgboost-2.0.2:\n","      Successfully uninstalled xgboost-2.0.2\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.2.2\n","    Uninstalling scikit-learn-1.2.2:\n","      Successfully uninstalled scikit-learn-1.2.2\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.1.0+cu121\n","    Uninstalling torch-2.1.0+cu121:\n","      Successfully uninstalled torch-2.1.0+cu121\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","bigframes 0.16.0 requires scikit-learn>=1.2.2, but you have scikit-learn 1.1.3 which is incompatible.\n","torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n","torchdata 0.7.0 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n","torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n","torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed boto3-1.34.2 botocore-1.34.2 brevitas-0.8.0 certifi-2023.7.22 coloredlogs-15.0.1 concrete-ml-1.3.0 concrete-python-2.5.0rc1 dependencies-2.0.1 dill-0.3.7 fastapi-0.102.0 gitdb-4.0.11 gitpython-3.1.35 h11-0.14.0 humanfriendly-10.0 hummingbird-ml-0.4.8 jmespath-1.0.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 onnx-1.13.1 onnxconverter-common-1.13.0 onnxmltools-1.11.0 onnxoptimizer-0.3.10 onnxruntime-1.13.1 pytest-json-report-1.5.0 pytest-metadata-3.0.0 s3transfer-0.9.0 scikit-learn-1.1.3 scipy-1.10.1 setuptools-65.6.3 skl2onnx-1.12 skops-0.5.0 skorch-0.11.0 smmap-5.0.1 starlette-0.27.0 torch-1.13.1 uvicorn-0.21.1 xgboost-1.6.2 z3-solver-4.12.4.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["_distutils_hack","certifi","pkg_resources","setuptools"]}}},"metadata":{}}]},{"cell_type":"markdown","source":["### Utils"],"metadata":{"id":"GAlsQub2dIDf"}},{"cell_type":"code","source":["import pickle as pkl\n","import random\n","import sys\n","import warnings\n","from collections import OrderedDict\n","from pathlib import Path\n","from time import time\n","from typing import Callable, Dict, Optional, Tuple\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from brevitas import config\n","from concrete.fhe.compilation import Configuration\n","#from models import Fp32VGG11\n","from sklearn.metrics import top_k_accuracy_score\n","from torch.utils.data import Dataset\n","from torch.utils.data.dataloader import DataLoader\n","#from torchvision import datasets, transforms\n","#from torchvision.utils import make_grid\n","from tqdm import tqdm\n","\n","from concrete.ml.torch.compile import compile_brevitas_qat_model\n","\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","\n","class FromTensors(Dataset):\n","    #def __init__(self, images_path, labels_path):\n","    def __init__(self, dataset_path):\n","        #self.images = torch.load(images_path)\n","        #self.labels = torch.load(labels_path).tolist()\n","        self.images, self.labels = torch.load(dataset_path)\n","        self.labels = self.labels.tolist()\n","\n","    def __getitem__(self, index):\n","        return self.images[index], self.labels[index]\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","def load_torch_dataset(\n","        cluster_idx: int,\n","        base_path :str,\n","        train_set :bool) -> Dataset:\n","\n","    split = 'train' if train_set else 'test'\n","    ds_name = f'cluster{cluster_idx:02d}' if cluster_idx>=0 else 'cluster_cls'\n","    return FromTensors( f'{base_path}/{ds_name}_{split}.pt')\n","\n","def get_dataloader_precanned(\n","    param: Dict,\n",") -> Tuple[DataLoader, DataLoader]:\n","    \"\"\"Returns the training and the test loaders of either CIFAR-10 or CIFAR-100 data-set.\n","    Args:\n","        param (Dict): Set of hyper-parameters\n","    Return:\n","        Tuple[DataLoader, DataLoader]: Training and test data loaders.\n","    \"\"\"\n","    dataset_name = param[\"dataset_name\"]\n","    path = param[\"dir\"]\n","\n","    if param['train_required']:\n","        train_dataset = load_torch_dataset(param[\"cluster_idx\"], path, True)\n","\n","        train_loader = torch.utils.data.DataLoader(\n","            train_dataset,\n","            batch_size= param[\"batch_size\"],\n","            shuffle=True,\n","            drop_last=True,\n","        )\n","    else:\n","        train_loader = None\n","\n","    if param['test_required']:\n","        test_dataset = load_torch_dataset(param[\"cluster_idx\"], path, False)\n","\n","        test_loader = torch.utils.data.DataLoader(\n","            test_dataset,\n","            batch_size= param[\"batch_size\"],\n","            shuffle=True,\n","            drop_last=True,\n","        )\n","    else:\n","        test_loader = None\n","\n","    return (train_loader, test_loader)\n","\n","\n","def plot_history(param: Dict, load: bool = False) -> None:\n","    \"\"\"Display the loss and accuracy for the test and training sets.\n","\n","    Args:\n","        param (Dict): Set of hyper-parameters to use depending on whether\n","            CIFAR-10 or CIFAR-100 is used.\n","        load (bool): If True, we upload the stored param.\n","    Returns:\n","        None.\n","    \"\"\"\n","\n","    if load:\n","        with open(\n","            f\"{param['dir']}/{param['training']}/{param['dataset_name']}_history.pkl\", \"br\"\n","        ) as f:\n","            param = pkl.load(f)\n","\n","    fig, axes = plt.subplots(1, 2, figsize=(15, 8))\n","\n","    # Plot the training and test loss.\n","    axes[0].plot(\n","        range(len(param[\"loss_train_history\"])),\n","        param[\"loss_train_history\"],\n","        label=\"Train loss\",\n","        marker=\"o\",\n","    )\n","    axes[0].plot(\n","        range(len(param[\"loss_test_history\"])),\n","        param[\"loss_test_history\"],\n","        label=\"Test loss\",\n","        marker=\"s\",\n","    )\n","\n","    axes[0].set_title(\"Loss\")\n","    axes[0].set_ylabel(\"Loss\")\n","    axes[0].set_xlabel(\"Epochs\")\n","    axes[0].legend(loc=\"best\")\n","    axes[0].grid(True)\n","\n","    # Plot the training and test accuracy.\n","    axes[1].plot(\n","        range(len(param[\"accuracy_train\"])),\n","        param[\"accuracy_train\"],\n","        label=\"Train accuracy\",\n","        marker=\"o\",\n","    )\n","    axes[1].plot(\n","        range(len(param[\"accuracy_test\"])),\n","        param[\"accuracy_test\"],\n","        label=\"Test accuracy\",\n","        marker=\"s\",\n","    )\n","\n","    axes[1].set_title(\"Top-1 accuracy\")\n","    axes[1].set_ylabel(\"Accuracy\")\n","    axes[1].set_xlabel(\"Epochs\")\n","    axes[1].legend(loc=\"best\")\n","    axes[1].grid(True)\n","\n","    fig.tight_layout()\n","    plt.show()\n","\n","\n","def plot_baseline(param: Dict, data: DataLoader, device: str) -> None:\n","    \"\"\"\n","    Display the test accuracy given `param` arguments\n","    that we got using Transfer Learning and QAT approaches.\n","\n","    Args:\n","        param (Dict): Set of hyper-parameters to use depending on whether\n","            CIFAR-10 or CIFAR-100 is used.\n","        data (DataLoader): Test set.\n","        device (str): Device type.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    # The accuracy of the counterpart pre-trained model in fp32 will be used as a baseline.\n","    # That we try to catch up during the Quantization Aware Training.\n","    checkpoint = torch.load(f\"{param['dir']}/{param['pre_trained_path']}\", map_location=device)\n","    fp32_vgg = Fp32VGG11(param[\"output_size\"])\n","    fp32_vgg.load_state_dict(checkpoint)\n","    baseline = torch_inference(fp32_vgg, data, param, device)\n","\n","    plt.plot(\n","        range(len(param[\"accuracy_test\"])),\n","        param[\"accuracy_test\"],\n","        marker=\"o\",\n","        label=\"Test accuracy\",\n","    )\n","    plt.text(x=0, y=baseline + 0.01, s=f\"Baseline = {baseline * 100: 2.2f}%\", fontsize=15, c=\"red\")\n","    plt.plot(range(len(param[\"accuracy_test\"])), [baseline] * len(param[\"accuracy_test\"]), \"r--\")\n","\n","    plt.title(f\"Accuracy on the testing set with {param['dataset_name']}\")\n","    plt.legend(loc=\"best\")\n","    plt.ylabel(\"Accuracy\")\n","    plt.xlabel(\"Epochs\")\n","    plt.xlim(-0.3, 4.2)\n","    plt.ylim(0, 1)\n","    plt.grid(True)\n","    plt.show()\n","\n","\n","def train(\n","    model: nn.Module,\n","    train_loader: DataLoader,\n","    test_loader: DataLoader,\n","    param: Dict,\n","    step: int = 1,\n","    device: str = \"cpu\",\n",") -> nn.Module:\n","    \"\"\"Training the model.\n","\n","    Args:\n","        model (nn.Module): A PyTorch or Brevitas network.\n","        train_loader (DataLoader): The training set.\n","        test_loader (DataLoader): The test set.\n","        param (Dict): Set of hyper-parameters to use depending on whether\n","            CIFAR-10 or CIFAR-100 is used.\n","        step (int): Display the loss and accuracy every `epoch % step`.\n","        device (str): Device type.\n","    Returns:\n","        nn.Module: the trained model.\n","    \"\"\"\n","\n","    torch.manual_seed(param[\"seed\"])\n","    random.seed(param[\"seed\"])\n","\n","    model = model.to(device)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=param[\"lr\"])\n","    scheduler = torch.optim.lr_scheduler.MultiStepLR(\n","        optimizer, milestones=param[\"milestones\"], gamma=param[\"gamma\"]\n","    )\n","\n","    # To avoid breaking up the tqdm bar\n","    with tqdm(total=param[\"epochs\"], file=sys.stdout) as pbar:\n","\n","        for i in range(param[\"epochs\"]):\n","            # Train the model\n","            model.train()\n","            loss_batch_train, accuracy_batch_train = [], []\n","\n","            for x, y in train_loader:\n","                x, y = x.to(device), y.to(device)\n","\n","                optimizer.zero_grad()\n","                yhat = model(x)\n","                loss_train = param[\"criterion\"](yhat, y)\n","                loss_train.backward()\n","                optimizer.step()\n","\n","                loss_batch_train.append(loss_train.item())\n","                accuracy_batch_train.extend((yhat.argmax(1) == y).cpu().float().tolist())\n","\n","            if scheduler:\n","                scheduler.step()\n","\n","            param[\"accuracy_train\"].append(np.mean(accuracy_batch_train))\n","            param[\"loss_train_history\"].append(np.mean(loss_batch_train))\n","\n","            # Evaluation during training:\n","            # Disable autograd engine (no backpropagation)\n","            # To reduce memory usage and to speed up computations\n","            with torch.no_grad():\n","                # Notify batchnormalization & dropout layers to work in eval mode\n","                model.eval()\n","                loss_batch_test, accuracy_batch_test = [], []\n","                for x, y in test_loader:\n","                    x, y = x.to(device), y.to(device)\n","                    yhat = model(x)\n","                    loss_test = param[\"criterion\"](yhat, y)\n","                    loss_batch_test.append(loss_test.item())\n","                    accuracy_batch_test.extend((yhat.argmax(1) == y).cpu().float().tolist())\n","\n","                param[\"accuracy_test\"].append(np.mean(accuracy_batch_test))\n","                param[\"loss_test_history\"].append(np.mean(loss_batch_test))\n","\n","            if i % step == 0:\n","                pbar.write(\n","                    f\"Epoch {i:2}: Train loss = {param['loss_train_history'][-1]:.4f} \"\n","                    f\"VS Test loss = {param['loss_test_history'][-1]:.4f} - \"\n","                    f\"Accuracy train: {param['accuracy_train'][-1]:.4f} \"\n","                    f\"VS Accuracy test: {param['accuracy_test'][-1]:.4f}\"\n","                )\n","                pbar.update(step)\n","\n","    # Save the state_dict\n","    dir = Path(\".\") / param[\"dir\"] / param[\"training\"]\n","    dir.mkdir(parents=True, exist_ok=True)\n","    torch.save(\n","        model.state_dict(), f\"{dir}/{param['dataset_name']}_{param['training']}_state_dict.pt\"\n","    )\n","\n","    with open(f\"{dir}/{param['dataset_name']}_history.pkl\", \"wb\") as f:\n","        pkl.dump(param, f)\n","\n","    torch.cuda.empty_cache()\n","\n","    return model\n","\n","\n","def torch_inference(\n","    model: nn.Module,\n","    data: DataLoader,\n","    device: str = \"cpu\",\n","    verbose: bool = False,\n",") -> float:\n","\n","    \"\"\"Returns the `top_k` accuracy.\n","\n","    Args:\n","        model (nn.Module): A PyTorch or Brevitas network.\n","        data (DataLoader): The test or evaluation set.\n","        device (str): Device type.\n","        verbose (bool): For display.\n","    Returns:\n","        float: The top_k accuracy.\n","    \"\"\"\n","    correct = []\n","    total_example = 0\n","    model = model.to(device)\n","\n","    with torch.no_grad():\n","        model.eval()\n","        for x, y in tqdm(data, disable=verbose is False):\n","            x, y = x.to(device), y\n","            yhat = model(x).cpu().detach()\n","            correct.append(yhat.argmax(1) == y)\n","            total_example += len(x)\n","\n","    return np.mean(np.vstack(correct), dtype=\"float64\")\n","\n","\n","def fhe_compatibility(model: Callable, data: DataLoader) -> Callable:\n","    \"\"\"Test if the model is FHE-compatible.\n","\n","    Args:\n","        model (Callable): The Brevitas model.\n","        data (DataLoader): The data loader.\n","\n","    Returns:\n","        Callable: Quantized model.\n","    \"\"\"\n","\n","    qmodel = compile_brevitas_qat_model(\n","        model.to(\"cpu\"),\n","        # Training\n","        torch_inputset=data,\n","        show_mlir=False,\n","        output_onnx_file=\"test.onnx\",\n","    )\n","\n","    return qmodel\n","\n","\n","def mapping_keys(pre_trained_weights: Dict, model: nn.Module, device: str) -> nn.Module:\n","\n","    \"\"\"\n","    Initialize the quantized model with pre-trained fp32 weights.\n","\n","    Args:\n","        pre_trained_weights (Dict): The state_dict of the pre-trained fp32 model.\n","        model (nn.Module): The Brevitas model.\n","        device (str): Device type.\n","\n","    Returns:\n","        Callable: The quantized model with the pre-trained state_dict.\n","    \"\"\"\n","\n","    # Brevitas requirement to ignore missing keys\n","    config.IGNORE_MISSING_KEYS = True\n","\n","    old_keys = list(pre_trained_weights.keys())\n","    new_keys = list(model.state_dict().keys())\n","    new_state_dict = OrderedDict()\n","\n","    for old_key, new_key in zip(old_keys, new_keys):\n","        new_state_dict[new_key] = pre_trained_weights[old_key]\n","\n","    model.load_state_dict(new_state_dict)\n","    model = model.to(device)\n","\n","    return model\n","\n","\n","def fhe_simulation_inference(quantized_module, data_loader, verbose: bool = False) -> float:\n","    \"\"\"Evaluate the model in FHE simulation mode.\n","\n","    Args:\n","        quantized_module (Callable): The quantized module.\n","        data_loader (int): The test or evaluation set.\n","        verbose (bool): For display.\n","\n","    Returns:\n","        float: The accuracy measured through FHE simulation\n","    \"\"\"\n","    correct_sim = []\n","    total_example = 0\n","\n","    disable_tqdm = not verbose\n","    for data, labels in tqdm(data_loader, disable=disable_tqdm):\n","\n","        data, labels = data.detach().cpu().numpy(), labels.detach().cpu().numpy()\n","\n","        # Store the predicted quantized probabilities\n","        predictions = quantized_module.forward(data, fhe=\"simulate\")\n","\n","        total_example += data.shape[0]\n","\n","        # Store the class predictions\n","        correct_sim.extend(predictions.argmax(1) == labels)\n","\n","    acc = np.mean(correct_sim, dtype=\"float64\")\n","    return acc"],"metadata":{"id":"_ljS_VhvdOlh","executionInfo":{"status":"ok","timestamp":1702760382350,"user_tz":-120,"elapsed":2984,"user":{"displayName":"Arie Genkin","userId":"17117459664307926572"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["### Models"],"metadata":{"id":"dI2bPG2Ddah9"}},{"cell_type":"code","source":["import brevitas\n","import brevitas.nn as qnn\n","import torch\n","import torch.nn as nn\n","from brevitas.quant import Int8ActPerTensorFloat, Int8WeightPerTensorFloat\n","\n","\"\"\" In this models.py we provide the code for the PyTorch and Brevitas networks.\"\"\"\n","\n","# This architecture is inspired by the original VGG-11 network available in\n","# PyTorch.hub (https://pytorch.org/hub/pytorch_vision_vgg/)\n","\n","# Each tuple refers to a PyTorch or Brevitas layer:\n","# I: QuantIdentity layer, only required for the Brevitas network. Mainly used to quantize\n","# the input data or to encapsulate a PyTorch layer inside the Brevitas model.\n","# C: Convolutional layer.\n","# P: Pooling layer, we replaced the original `MaxPool2d` in VGG-11 by a `AvgPool2d` layer.\n","# Because in the current version of Concrete ML `MaxPool2d` isn't available yet.\n","# R: ReLU activation.\n","FEATURES_MAPS = [\n","    (\"I\",),\n","    (\"C\", 3, 64, 3, 1, 1),\n","    (\"R\",),\n","    (\"P\", 2, 2, 0, 1, False),\n","    (\"I\",),\n","    (\"C\", 64, 128, 3, 1, 1),\n","    (\"R\",),\n","    (\"P\", 2, 2, 0, 1, False),\n","    (\"I\",),\n","    (\"C\", 128, 256, 3, 1, 1),\n","    (\"R\",),\n","    (\"C\", 256, 256, 3, 1, 1),\n","    (\"R\",),\n","    (\"P\", 2, 2, 0, 1, False),\n","    (\"I\",),\n","    (\"C\", 256, 512, 3, 1, 1),\n","    (\"R\",),\n","    (\"C\", 512, 512, 3, 1, 1),\n","    (\"R\",),\n","    (\"P\", 2, 2, 0, 1, False),\n","    (\"I\",),\n","    (\"C\", 512, 512, 3, 1, 1),\n","    (\"R\",),\n","    (\"C\", 512, 512, 3, 1, 1),\n","    (\"R\",),\n","    (\"P\", 2, 2, 0, 1, False),\n","    (\"I\",),\n","    (\"P\", 2, 2, 0, 1, False),  # this and the next layer are needed in QuantVGG11 but not in Fp32VGG11\n","    (\"I\",),\n","    ]\n","\n","class QuantVGG11(nn.Module):\n","    def __init__(\n","        self,\n","        bit: int,\n","        output_size: int = 3,\n","        act_quant: brevitas.quant = Int8ActPerTensorFloat,\n","        weight_quant: brevitas.quant = Int8WeightPerTensorFloat,\n","    ):\n","        \"\"\"A quantized network with Brevitas.\n","\n","        Args:\n","            bit (int): Bit of quantization.\n","            output_size (int): Number of classes.\n","            act_quant (brevitas.quant): Quantization protocol of activations.\n","            weight_quant (brevitas.quant): Quantization protocol of the weights.\n","\n","        \"\"\"\n","        super(QuantVGG11, self).__init__()\n","        self.bit = bit\n","\n","        def tuple2quantlayer(t):\n","            if t[0] == \"R\":\n","                return qnn.QuantReLU(return_quant_tensor=True, bit_width=bit, act_quant=act_quant)\n","            if t[0] == \"P\":\n","                return nn.AvgPool2d(kernel_size=t[1], stride=t[2], padding=t[3], ceil_mode=t[5])\n","            if t[0] == \"C\":\n","                return qnn.QuantConv2d(\n","                    t[1],\n","                    t[2],\n","                    kernel_size=t[3],\n","                    stride=t[4],\n","                    padding=t[5],\n","                    weight_bit_width=bit,\n","                    weight_quant=weight_quant,\n","                    return_quant_tensor=True,\n","                )\n","            if t[0] == \"L\":\n","                return qnn.QuantLinear(\n","                    in_features=t[1],\n","                    out_features=t[2],\n","                    weight_bit_width=bit,\n","                    weight_quant=weight_quant,\n","                    bias=True,\n","                    return_quant_tensor=True,\n","                )\n","            if t[0] == \"I\":\n","                # According to the literature, the first layer holds the most information\n","                # about the input data. So, it is possible to quantize the input using more\n","                # precision bit-width than the rest of the network.\n","                identity_quant = t[1] if len(t) == 2 else bit\n","                return qnn.QuantIdentity(\n","                    bit_width=identity_quant, act_quant=act_quant, return_quant_tensor=True\n","                )\n","\n","        # The very first layer is a `QuantIdentity` layer, which is very important\n","        # to ensure that the input data is also quantized.\n","        self.features = nn.Sequential(*[tuple2quantlayer(t) for t in FEATURES_MAPS])\n","\n","        # self.identity1 and self.identity2 are used to encapsulate the `torch.flatten`.\n","        self.identity1 = qnn.QuantIdentity(\n","            bit_width=bit, act_quant=act_quant, return_quant_tensor=True\n","        )\n","\n","        self.identity2 = qnn.QuantIdentity(\n","            bit_width=bit, act_quant=act_quant, return_quant_tensor=True\n","        )\n","\n","        # Fully connected linear layer.\n","        self.final_layer = qnn.QuantLinear(\n","            in_features=512 * 3 * 3,\n","            out_features=output_size,\n","            weight_quant=weight_quant,\n","            weight_bit_width=bit,\n","            bias=True,\n","            return_quant_tensor=True,\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.identity1(x)\n","        # As `torch.flatten` is a PyTorch layer, you must place it between two `QuantIdentity`\n","        # layers to ensure that all intermediate values of the network are properly quantized.\n","        x = torch.flatten(x, 1)\n","        # Replace `x.view(x.shape[0], -1)` by `torch.flatten(x, 1)` which is an equivalent\n","        # But is compatible with Concrete ML.\n","        x = self.identity2(x)\n","        x = self.final_layer(x)\n","        return x.value"],"metadata":{"id":"4aJ-ZGkUdoyB","executionInfo":{"status":"ok","timestamp":1702760382350,"user_tz":-120,"elapsed":6,"user":{"displayName":"Arie Genkin","userId":"17117459664307926572"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tMsJAKqqcLqn"},"source":["## Settings"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"7cf1k6L-cLqo","outputId":"7a341335-e602-4b48-fc02-1a1eaed1b3bb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702760382351,"user_tz":-120,"elapsed":6,"user":{"displayName":"Arie Genkin","userId":"17117459664307926572"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Device Type: cuda\n"]}],"source":["import torch\n","\n","bit = 5\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","print(f\"Device Type: {device}\")"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o45Bltjy0WNU","executionInfo":{"status":"ok","timestamp":1702760406745,"user_tz":-120,"elapsed":20266,"user":{"displayName":"Arie Genkin","userId":"17117459664307926572"}},"outputId":"53883114-5b98-4d01-ec84-23bb37a163c3"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"Urz1RXJXcLqq"},"source":["## Quantization Aware Training (QAT)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"iZ4qWIVEcLqr","executionInfo":{"status":"ok","timestamp":1702760412509,"user_tz":-120,"elapsed":526,"user":{"displayName":"Arie Genkin","userId":"17117459664307926572"}}},"outputs":[],"source":["param = {\n","    \"output_size\": 105,\n","    \"batch_size\": 16,\n","    \"training\": \"quant\",\n","    \"cluster_idx\": 0,\n","    \"dataset_name\": \"cluster00_large_150\",\n","    \"mean\": [0.485, 0.456, 0.406],\n","    \"std\": [0.229, 0.224, 0.225],\n","    \"lr\": 6e-5,\n","    \"seed\": 727,\n","    \"epochs\": 10,\n","    \"gamma\": 0.01,\n","    \"milestones\": [3, 5],\n","    \"criterion\": torch.nn.CrossEntropyLoss(),\n","    \"accuracy_test\": [],\n","    \"accuracy_train\": [],\n","    \"loss_test_history\": [],\n","    \"loss_train_history\": [],\n","    \"dir\": \"/content/drive/MyDrive/Colab Notebooks/Zama/f32_pretrained\",\n","    \"pre_trained_model\": \"cluster00_SPEC_fp32__state_dict.pt\",\n","    'train_required': True,\n","    'test_required': True,\n","}"]},{"cell_type":"markdown","source":["## Quant aware classiication training\n","- Clusters from 0 to num_clusters (we use num_clusters=80)\n","- cluster_idx = -1 is a special case, it indicates Cluster classification dataset and model\n"],"metadata":{"id":"UPQdqGeXEP_c"}},{"cell_type":"code","source":["for cluster_idx in range(-1,0):\n","    param[\"cluster_idx\"] = cluster_idx\n","    param[\"dataset_name\"] = f\"cluster{cluster_idx:02d}_large_150\" if cluster_idx >= 0 else 'cluster_cls'\n","    param[\"pre_trained_model\"] = f\"cluster{cluster_idx:02d}_SPEC_fp32_state_dict.pt\" if cluster_idx >= 0 else f\"cluster_cls_SPEC_fp32_state_dict.pt\"\n","\n","    # Load dataset\n","    train_loader, test_loader = get_dataloader_precanned(param= param)\n","\n","    # fp32 pretrained model\n","    checkpoint = torch.load(f\"{param['dir']}/{param['pre_trained_model']}\", map_location=device)\n","    param[\"output_size\"] = checkpoint['final_layer.bias'].shape.numel()\n","    print(f\"Cluster {cluster_idx:02d} output size = {param['output_size']}\")\n","\n","    # Quantized model\n","    quant_vgg = QuantVGG11(bit=bit, output_size=param[\"output_size\"])\n","\n","    # Mapping keys from pretrained to quantized model\n","    quant_vgg = mapping_keys(checkpoint, quant_vgg, device)\n","\n","    # Train\n","    quant_vgg = train(quant_vgg, train_loader, test_loader, param, device=device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mZW1JIM6EORR","executionInfo":{"status":"ok","timestamp":1702750428188,"user_tz":-120,"elapsed":2977474,"user":{"displayName":"Arie Genkin","userId":"17117459664307926572"}},"outputId":"fd78dbb4-cd84-4616-b861-5c46444f3d17"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Cluster -1 output size = 80\n","Epoch  0: Train loss = 3.3306 VS Test loss = 2.9646 - Accuracy train: 0.1624 VS Accuracy test: 0.2262\n","Epoch  1: Train loss = 2.8746 VS Test loss = 2.8378 - Accuracy train: 0.2337 VS Accuracy test: 0.2367\n","Epoch  2: Train loss = 2.6408 VS Test loss = 2.7533 - Accuracy train: 0.2873 VS Accuracy test: 0.2519\n","Epoch  3: Train loss = 2.2218 VS Test loss = 2.6132 - Accuracy train: 0.3854 VS Accuracy test: 0.2902\n","Epoch  4: Train loss = 2.1421 VS Test loss = 2.5947 - Accuracy train: 0.4041 VS Accuracy test: 0.2982\n","Epoch  5: Train loss = 2.1166 VS Test loss = 2.5970 - Accuracy train: 0.4131 VS Accuracy test: 0.2925\n","Epoch  6: Train loss = 2.1139 VS Test loss = 2.5962 - Accuracy train: 0.4133 VS Accuracy test: 0.2917\n","Epoch  7: Train loss = 2.1130 VS Test loss = 2.5971 - Accuracy train: 0.4135 VS Accuracy test: 0.2942\n","Epoch  8: Train loss = 2.1129 VS Test loss = 2.5956 - Accuracy train: 0.4126 VS Accuracy test: 0.2913\n","Epoch  9: Train loss = 2.1152 VS Test loss = 2.5961 - Accuracy train: 0.4116 VS Accuracy test: 0.2962\n","100%|██████████| 10/10 [41:50<00:00, 251.00s/it]\n"]}]},{"cell_type":"markdown","source":["## End to End test"],"metadata":{"id":"kohsJTVnJZFl"}},{"cell_type":"code","source":["from copy import deepcopy\n","\n","def instantiate_model(model_path):\n","    # Load cluster classification model state_dict\n","    state_dict = torch.load( model_path, map_location=device)\n","    output_size = state_dict['final_layer.bias'].shape.numel()\n","\n","    # Create model template\n","    model = QuantVGG11(bit=bit, output_size=output_size)\n","\n","    # Copy the trained state_dict\n","    model.load_state_dict(deepcopy(state_dict), strict=False)\n","    # Move model to device\n","    model = model.to(device)\n","\n","    return model"],"metadata":{"id":"KfHALwsuU5aS","executionInfo":{"status":"ok","timestamp":1702760505992,"user_tz":-120,"elapsed":516,"user":{"displayName":"Arie Genkin","userId":"17117459664307926572"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","\n","results = []\n","\n","# Instantiate cluster classification model\n","cluster_cls_model = instantiate_model(\"/content/drive/MyDrive/Colab Notebooks/Zama/f32_pretrained/quant/cluster_cls_quant_state_dict.pt\")\n","\n","# Test for available clusters (ones having intra-cluster classificaton models trained)\n","for cluster_idx in range(0, 6):\n","    param[\"cluster_idx\"] = cluster_idx\n","    param['batch_size'] = 1\n","    param['train_required'] = False\n","    param['test_required'] = True\n","\n","    # Instantiate intra-cluster track classification model\n","    intra_cluster_model = instantiate_model(f\"/content/drive/MyDrive/Colab Notebooks/Zama/f32_pretrained/quant/cluster{cluster_idx:02d}_large_150_quant_state_dict.pt\")\n","\n","    # Test with various augmentation layers\n","    for augm in ['small', 'medium', 'large']:\n","\n","        total_samples = 0\n","        top1_cls_correct, top1_e2e_correct = 0, 0\n","        top3_cls_correct, top3_e2e_correct = 0, 0\n","\n","        param['dir'] = f\"/content/drive/MyDrive/Colab Notebooks/Zama/e2e_test_data/{augm}\"\n","        param[\"dataset_name\"] = f\"cluster{cluster_idx:02d}_large_150\" if cluster_idx >= 0 else 'cluster_cls'\n","\n","        # Load dataset\n","        _, test_loader = get_dataloader_precanned(param= param)   # only test end2end datasets\n","\n","        # Run intra-cluster track classification inference\n","        # Collect all correctly classfied tracks (within the given cluster)\n","        intra_cluster_correct = []\n","        with torch.no_grad():\n","            intra_cluster_model.eval()\n","            for x, track_id in test_loader:\n","                x = x.to(device)\n","                track_preds = intra_cluster_model(x).cpu().detach()\n","                track_preds = track_preds.numpy()\n","\n","                total_samples += 1\n","                if track_id == np.argmax(track_preds):\n","                    intra_cluster_correct.append(track_id)\n","                break\n","\n","        # Run cluster classification inference\n","        with torch.no_grad():\n","            cluster_cls_model.eval()\n","            for x, track_id in test_loader:\n","                x = x.to(device)\n","                cluster_preds = cluster_cls_model(x).cpu().detach()\n","                cluster_preds = cluster_preds.numpy()\n","\n","                if cluster_idx == np.argmax(cluster_preds):\n","                    top1_cls_correct += 1\n","                if cluster_idx == np.argmax(cluster_preds) and track_id in intra_cluster_correct:\n","                    top1_e2e_correct += 1\n","                if cluster_idx in np.argsort(cluster_preds)[-3:]:\n","                    top3_cls_correct += 1\n","                if cluster_idx in np.argsort(cluster_preds)[-3:] and track_id in intra_cluster_correct:\n","                    top3_e2e_correct += 1\n","                break\n","\n","        results.append({\n","            \"cluster\": cluster_idx,\n","            \"augmentation\": augm,\n","            \"top1_cls_correct\": int(top1_cls_correct/total_samples*100),\n","            \"top3_cls_correct\": int(top3_cls_correct/total_samples*100),\n","            \"top1_e2e_correct\": int(top1_e2e_correct/total_samples*100),\n","            \"top3_e2e_correct\": int(top3_e2e_correct/total_samples*100)\n","        })\n","\n","df = pd.DataFrame(results, columns=[\"cluster\", \"augmentation\", \"top1_cls_correct\", \"top3_cls_correct\", \"top1_e2e_correct\", \"top3_e2e_correct\"])\n","df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/Zama/e2e_test_results.csv\", index=False)\n"],"metadata":{"id":"gWlt8tbxsebL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Backup"],"metadata":{"id":"fMb0YyGPsexq"}},{"cell_type":"code","source":["!pip install torchinfo\n","from torchinfo import summary"],"metadata":{"id":"vgRo5YwypB1u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["quant_vgg = QuantVGG11(bit=bit, output_size=105)\n","summary(quant_vgg, input_size=[16, 3, 224, 224])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LG9IHImrnc6O","executionInfo":{"status":"ok","timestamp":1702497685452,"user_tz":-120,"elapsed":3409,"user":{"displayName":"Arie Genkin","userId":"17117459664307926572"}},"outputId":"e8fb83d5-f6f2-4551-f7fc-526df1cc93e3"},"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["========================================================================================================================\n","Layer (type:depth-idx)                                                 Output Shape              Param #\n","========================================================================================================================\n","QuantVGG11                                                             [16, 105]                 --\n","├─Sequential: 1-1                                                      [16, 512, 3, 3]           9,220,494\n","│    └─QuantIdentity: 2-1                                              [16, 3, 224, 224]         --\n","│    │    └─ActQuantProxyFromInjector: 3-1                             [16, 3, 224, 224]         --\n","│    │    └─ActQuantProxyFromInjector: 3-2                             [16, 3, 224, 224]         1\n","├─QuantIdentity: 1-30                                                  --                        (recursive)\n","│    └─ActQuantProxyFromInjector: 2-58                                 --                        (recursive)\n","│    │    └─FusedActivationQuantProxy: 3-91                            --                        (recursive)\n","├─Sequential: 1-31                                                     --                        (recursive)\n","│    └─QuantIdentity: 2-3                                              --                        (recursive)\n","│    │    └─ActQuantProxyFromInjector: 3-4                             --                        (recursive)\n","│    └─QuantConv2d: 2-4                                                [16, 64, 224, 224]        64\n","│    │    └─ActQuantProxyFromInjector: 3-5                             [16, 3, 224, 224]         --\n","│    │    └─WeightQuantProxyFromInjector: 3-6                          [64, 3, 3, 3]             1,728\n","│    │    └─BiasQuantProxyFromInjector: 3-7                            [64]                      --\n","│    │    └─ActQuantProxyFromInjector: 3-8                             [16, 64, 224, 224]        --\n","│    └─QuantReLU: 2-5                                                  [16, 64, 224, 224]        --\n","│    │    └─ActQuantProxyFromInjector: 3-9                             [16, 64, 224, 224]        --\n","│    │    └─ActQuantProxyFromInjector: 3-10                            [16, 64, 224, 224]        1\n","├─QuantIdentity: 1-30                                                  --                        (recursive)\n","│    └─ActQuantProxyFromInjector: 2-58                                 --                        (recursive)\n","│    │    └─FusedActivationQuantProxy: 3-91                            --                        (recursive)\n","├─Sequential: 1-31                                                     --                        (recursive)\n","│    └─QuantReLU: 2-7                                                  --                        (recursive)\n","│    │    └─ActQuantProxyFromInjector: 3-12                            --                        (recursive)\n","│    └─AvgPool2d: 2-8                                                  [16, 64, 112, 112]        --\n","│    └─QuantIdentity: 2-9                                              [16, 64, 112, 112]        --\n","│    │    └─ActQuantProxyFromInjector: 3-13                            [16, 64, 112, 112]        --\n","│    │    └─ActQuantProxyFromInjector: 3-14                            [16, 64, 112, 112]        1\n","├─QuantIdentity: 1-30                                                  --                        (recursive)\n","│    └─ActQuantProxyFromInjector: 2-58                                 --                        (recursive)\n","│    │    └─FusedActivationQuantProxy: 3-91                            --                        (recursive)\n","├─Sequential: 1-31                                                     --                        (recursive)\n","│    └─QuantIdentity: 2-11                                             --                        (recursive)\n","│    │    └─ActQuantProxyFromInjector: 3-16                            --                        (recursive)\n","│    └─QuantConv2d: 2-12                                               [16, 128, 112, 112]       128\n","│    │    └─ActQuantProxyFromInjector: 3-17                            [16, 64, 112, 112]        --\n","│    │    └─WeightQuantProxyFromInjector: 3-18                         [128, 64, 3, 3]           73,728\n","│    │    └─BiasQuantProxyFromInjector: 3-19                           [128]                     --\n","│    │    └─ActQuantProxyFromInjector: 3-20                            [16, 128, 112, 112]       --\n","│    └─QuantReLU: 2-13                                                 [16, 128, 112, 112]       --\n","│    │    └─ActQuantProxyFromInjector: 3-21                            [16, 128, 112, 112]       --\n","│    │    └─ActQuantProxyFromInjector: 3-22                            [16, 128, 112, 112]       1\n","├─QuantIdentity: 1-30                                                  --                        (recursive)\n","│    └─ActQuantProxyFromInjector: 2-58                                 --                        (recursive)\n","│    │    └─FusedActivationQuantProxy: 3-91                            --                        (recursive)\n","├─Sequential: 1-31                                                     --                        (recursive)\n","│    └─QuantReLU: 2-15                                                 --                        (recursive)\n","│    │    └─ActQuantProxyFromInjector: 3-24                            --                        (recursive)\n","│    └─AvgPool2d: 2-16                                                 [16, 128, 56, 56]         --\n","│    └─QuantIdentity: 2-17                                             [16, 128, 56, 56]         --\n","│    │    └─ActQuantProxyFromInjector: 3-25                            [16, 128, 56, 56]         --\n","│    │    └─ActQuantProxyFromInjector: 3-26                            [16, 128, 56, 56]         1\n","├─QuantIdentity: 1-30                                                  --                        (recursive)\n","│    └─ActQuantProxyFromInjector: 2-58                                 --                        (recursive)\n","│    │    └─FusedActivationQuantProxy: 3-91                            --                        (recursive)\n","├─Sequential: 1-31                                                     --                        (recursive)\n","│    └─QuantIdentity: 2-19                                             --                        (recursive)\n","│    │    └─ActQuantProxyFromInjector: 3-28                            --                        (recursive)\n","│    └─QuantConv2d: 2-20                                               [16, 256, 56, 56]         256\n","│    │    └─ActQuantProxyFromInjector: 3-29                            [16, 128, 56, 56]         --\n","│    │    └─WeightQuantProxyFromInjector: 3-30                         [256, 128, 3, 3]          294,912\n","│    │    └─BiasQuantProxyFromInjector: 3-31                           [256]                     --\n","│    │    └─ActQuantProxyFromInjector: 3-32                            [16, 256, 56, 56]         --\n","│    └─QuantReLU: 2-21                                                 [16, 256, 56, 56]         --\n","│    │    └─ActQuantProxyFromInjector: 3-33                            [16, 256, 56, 56]         --\n","│    │    └─ActQuantProxyFromInjector: 3-34                            [16, 256, 56, 56]         1\n","├─QuantIdentity: 1-30                                                  --                        (recursive)\n","│    └─ActQuantProxyFromInjector: 2-58                                 --                        (recursive)\n","│    │    └─FusedActivationQuantProxy: 3-91                            --                        (recursive)\n","├─Sequential: 1-31                                                     --                        (recursive)\n","│    └─QuantReLU: 2-23                                                 --                        (recursive)\n","│    │    └─ActQuantProxyFromInjector: 3-36                            --                        (recursive)\n","│    └─QuantConv2d: 2-24                                               [16, 256, 56, 56]         256\n","│    │    └─ActQuantProxyFromInjector: 3-37                            [16, 256, 56, 56]         --\n","│    │    └─WeightQuantProxyFromInjector: 3-38                         [256, 256, 3, 3]          589,824\n","│    │    └─BiasQuantProxyFromInjector: 3-39                           [256]                     --\n","│    │    └─ActQuantProxyFromInjector: 3-40                            [16, 256, 56, 56]         --\n","│    └─QuantReLU: 2-25                                                 [16, 256, 56, 56]         --\n","│    │    └─ActQuantProxyFromInjector: 3-41                            [16, 256, 56, 56]         --\n","│    │    └─ActQuantProxyFromInjector: 3-42                            [16, 256, 56, 56]         1\n","├─QuantIdentity: 1-30                                                  --                        (recursive)\n","│    └─ActQuantProxyFromInjector: 2-58                                 --                        (recursive)\n","│    │    └─FusedActivationQuantProxy: 3-91                            --                        (recursive)\n","├─Sequential: 1-31                                                     --                        (recursive)\n","│    └─QuantReLU: 2-27                                                 --                        (recursive)\n","│    │    └─ActQuantProxyFromInjector: 3-44                            --                        (recursive)\n","│    └─AvgPool2d: 2-28                                                 [16, 256, 28, 28]         --\n","│    └─QuantIdentity: 2-29                                             [16, 256, 28, 28]         --\n","│    │    └─ActQuantProxyFromInjector: 3-45                            [16, 256, 28, 28]         --\n","│    │    └─ActQuantProxyFromInjector: 3-46                            [16, 256, 28, 28]         1\n","├─QuantIdentity: 1-30                                                  --                        (recursive)\n","│    └─ActQuantProxyFromInjector: 2-58                                 --                        (recursive)\n","│    │    └─FusedActivationQuantProxy: 3-91                            --                        (recursive)\n","├─Sequential: 1-31                                                     --                        (recursive)\n","│    └─QuantIdentity: 2-31                                             --                        (recursive)\n","│    │    └─ActQuantProxyFromInjector: 3-48                            --                        (recursive)\n","│    └─QuantConv2d: 2-32                                               [16, 512, 28, 28]         512\n","│    │    └─ActQuantProxyFromInjector: 3-49                            [16, 256, 28, 28]         --\n","│    │    └─WeightQuantProxyFromInjector: 3-50                         [512, 256, 3, 3]          1,179,648\n","│    │    └─BiasQuantProxyFromInjector: 3-51                           [512]                     --\n","│    │    └─ActQuantProxyFromInjector: 3-52                            [16, 512, 28, 28]         --\n","│    └─QuantReLU: 2-33                                                 [16, 512, 28, 28]         --\n","│    │    └─ActQuantProxyFromInjector: 3-53                            [16, 512, 28, 28]         --\n","│    │    └─ActQuantProxyFromInjector: 3-54                            [16, 512, 28, 28]         1\n","├─QuantIdentity: 1-30                                                  --                        (recursive)\n","│    └─ActQuantProxyFromInjector: 2-58                                 --                        (recursive)\n","│    │    └─FusedActivationQuantProxy: 3-91                            --                        (recursive)\n","├─Sequential: 1-31                                                     --                        (recursive)\n","│    └─QuantReLU: 2-35                                                 --                        (recursive)\n","│    │    └─ActQuantProxyFromInjector: 3-56                            --                        (recursive)\n","│    └─QuantConv2d: 2-36                                               [16, 512, 28, 28]         512\n","│    │    └─ActQuantProxyFromInjector: 3-57                            [16, 512, 28, 28]         --\n","│    │    └─WeightQuantProxyFromInjector: 3-58                         [512, 512, 3, 3]          2,359,296\n","│    │    └─BiasQuantProxyFromInjector: 3-59                           [512]                     --\n","│    │    └─ActQuantProxyFromInjector: 3-60                            [16, 512, 28, 28]         --\n","│    └─QuantReLU: 2-37                                                 [16, 512, 28, 28]         --\n","│    │    └─ActQuantProxyFromInjector: 3-61                            [16, 512, 28, 28]         --\n","│    │    └─ActQuantProxyFromInjector: 3-62                            [16, 512, 28, 28]         1\n","├─QuantIdentity: 1-30                                                  --                        (recursive)\n","│    └─ActQuantProxyFromInjector: 2-58                                 --                        (recursive)\n","│    │    └─FusedActivationQuantProxy: 3-91                            --                        (recursive)\n","├─Sequential: 1-31                                                     --                        (recursive)\n","│    └─QuantReLU: 2-39                                                 --                        (recursive)\n","│    │    └─ActQuantProxyFromInjector: 3-64                            --                        (recursive)\n","│    └─AvgPool2d: 2-40                                                 [16, 512, 14, 14]         --\n","│    └─QuantIdentity: 2-41                                             [16, 512, 14, 14]         --\n","│    │    └─ActQuantProxyFromInjector: 3-65                            [16, 512, 14, 14]         --\n","│    │    └─ActQuantProxyFromInjector: 3-66                            [16, 512, 14, 14]         1\n","├─QuantIdentity: 1-30                                                  --                        (recursive)\n","│    └─ActQuantProxyFromInjector: 2-58                                 --                        (recursive)\n","│    │    └─FusedActivationQuantProxy: 3-91                            --                        (recursive)\n","├─Sequential: 1-31                                                     --                        (recursive)\n","│    └─QuantIdentity: 2-43                                             --                        (recursive)\n","│    │    └─ActQuantProxyFromInjector: 3-68                            --                        (recursive)\n","│    └─QuantConv2d: 2-44                                               [16, 512, 14, 14]         512\n","│    │    └─ActQuantProxyFromInjector: 3-69                            [16, 512, 14, 14]         --\n","│    │    └─WeightQuantProxyFromInjector: 3-70                         [512, 512, 3, 3]          2,359,296\n","│    │    └─BiasQuantProxyFromInjector: 3-71                           [512]                     --\n","│    │    └─ActQuantProxyFromInjector: 3-72                            [16, 512, 14, 14]         --\n","│    └─QuantReLU: 2-45                                                 [16, 512, 14, 14]         --\n","│    │    └─ActQuantProxyFromInjector: 3-73                            [16, 512, 14, 14]         --\n","│    │    └─ActQuantProxyFromInjector: 3-74                            [16, 512, 14, 14]         1\n","├─QuantIdentity: 1-30                                                  --                        (recursive)\n","│    └─ActQuantProxyFromInjector: 2-58                                 --                        (recursive)\n","│    │    └─FusedActivationQuantProxy: 3-91                            --                        (recursive)\n","├─Sequential: 1-31                                                     --                        (recursive)\n","│    └─QuantReLU: 2-47                                                 --                        (recursive)\n","│    │    └─ActQuantProxyFromInjector: 3-76                            --                        (recursive)\n","│    └─QuantConv2d: 2-48                                               [16, 512, 14, 14]         512\n","│    │    └─ActQuantProxyFromInjector: 3-77                            [16, 512, 14, 14]         --\n","│    │    └─WeightQuantProxyFromInjector: 3-78                         [512, 512, 3, 3]          2,359,296\n","│    │    └─BiasQuantProxyFromInjector: 3-79                           [512]                     --\n","│    │    └─ActQuantProxyFromInjector: 3-80                            [16, 512, 14, 14]         --\n","│    └─QuantReLU: 2-49                                                 [16, 512, 14, 14]         --\n","│    │    └─ActQuantProxyFromInjector: 3-81                            [16, 512, 14, 14]         --\n","│    │    └─ActQuantProxyFromInjector: 3-82                            [16, 512, 14, 14]         1\n","├─QuantIdentity: 1-30                                                  --                        (recursive)\n","│    └─ActQuantProxyFromInjector: 2-58                                 --                        (recursive)\n","│    │    └─FusedActivationQuantProxy: 3-91                            --                        (recursive)\n","├─Sequential: 1-31                                                     --                        (recursive)\n","│    └─QuantReLU: 2-51                                                 --                        (recursive)\n","│    │    └─ActQuantProxyFromInjector: 3-84                            --                        (recursive)\n","│    └─AvgPool2d: 2-52                                                 [16, 512, 7, 7]           --\n","│    └─QuantIdentity: 2-53                                             [16, 512, 7, 7]           --\n","│    │    └─ActQuantProxyFromInjector: 3-85                            [16, 512, 7, 7]           --\n","│    │    └─ActQuantProxyFromInjector: 3-86                            [16, 512, 7, 7]           1\n","├─QuantIdentity: 1-30                                                  --                        (recursive)\n","│    └─ActQuantProxyFromInjector: 2-58                                 --                        (recursive)\n","│    │    └─FusedActivationQuantProxy: 3-91                            --                        (recursive)\n","├─Sequential: 1-31                                                     --                        (recursive)\n","│    └─QuantIdentity: 2-55                                             --                        (recursive)\n","│    │    └─ActQuantProxyFromInjector: 3-88                            --                        (recursive)\n","│    └─AvgPool2d: 2-56                                                 [16, 512, 3, 3]           --\n","│    └─QuantIdentity: 2-57                                             [16, 512, 3, 3]           --\n","│    │    └─ActQuantProxyFromInjector: 3-89                            [16, 512, 3, 3]           --\n","│    │    └─ActQuantProxyFromInjector: 3-90                            [16, 512, 3, 3]           1\n","├─QuantIdentity: 1-30                                                  --                        (recursive)\n","│    └─ActQuantProxyFromInjector: 2-58                                 --                        (recursive)\n","│    │    └─FusedActivationQuantProxy: 3-91                            --                        (recursive)\n","├─Sequential: 1-31                                                     --                        (recursive)\n","│    └─QuantIdentity: 2-59                                             --                        (recursive)\n","│    │    └─ActQuantProxyFromInjector: 3-92                            --                        (recursive)\n","├─QuantIdentity: 1-32                                                  [16, 4608]                --\n","│    └─ActQuantProxyFromInjector: 2-60                                 [16, 4608]                --\n","│    └─ActQuantProxyFromInjector: 2-61                                 [16, 4608]                --\n","│    │    └─FusedActivationQuantProxy: 3-93                            [16, 4608]                1\n","├─QuantLinear: 1-33                                                    [16, 105]                 105\n","│    └─ActQuantProxyFromInjector: 2-62                                 [16, 4608]                --\n","│    └─WeightQuantProxyFromInjector: 2-63                              [105, 4608]               --\n","│    │    └─RescalingIntQuant: 3-94                                    [105, 4608]               483,840\n","│    └─BiasQuantProxyFromInjector: 2-64                                [105]                     --\n","│    └─ActQuantProxyFromInjector: 2-65                                 [16, 105]                 --\n","========================================================================================================================\n","Total params: 18,924,935\n","Trainable params: 18,924,935\n","Non-trainable params: 0\n","Total mult-adds (M): 0\n","========================================================================================================================\n","Input size (MB): 9.63\n","Forward/backward pass size (MB): 0.00\n","Params size (MB): 0.00\n","Estimated Total Size (MB): 9.63\n","========================================================================================================================"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":["fp32_vgg = Fp32VGG11(output_size=105)\n","summary(fp32_vgg, input_size=[16, 3, 224, 224])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"25YJ1Zx-pEBM","executionInfo":{"status":"ok","timestamp":1702497292040,"user_tz":-120,"elapsed":1968,"user":{"displayName":"Arie Genkin","userId":"17117459664307926572"}},"outputId":"e8461bc8-a49e-429e-a943-02eaf1ec0e63"},"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","Fp32VGG11                                [16, 105]                 --\n","├─Sequential: 1-1                        [16, 512, 7, 7]           --\n","│    └─Conv2d: 2-1                       [16, 64, 224, 224]        1,792\n","│    └─ReLU: 2-2                         [16, 64, 224, 224]        --\n","│    └─AvgPool2d: 2-3                    [16, 64, 112, 112]        --\n","│    └─Conv2d: 2-4                       [16, 128, 112, 112]       73,856\n","│    └─ReLU: 2-5                         [16, 128, 112, 112]       --\n","│    └─AvgPool2d: 2-6                    [16, 128, 56, 56]         --\n","│    └─Conv2d: 2-7                       [16, 256, 56, 56]         295,168\n","│    └─ReLU: 2-8                         [16, 256, 56, 56]         --\n","│    └─Conv2d: 2-9                       [16, 256, 56, 56]         590,080\n","│    └─ReLU: 2-10                        [16, 256, 56, 56]         --\n","│    └─AvgPool2d: 2-11                   [16, 256, 28, 28]         --\n","│    └─Conv2d: 2-12                      [16, 512, 28, 28]         1,180,160\n","│    └─ReLU: 2-13                        [16, 512, 28, 28]         --\n","│    └─Conv2d: 2-14                      [16, 512, 28, 28]         2,359,808\n","│    └─ReLU: 2-15                        [16, 512, 28, 28]         --\n","│    └─AvgPool2d: 2-16                   [16, 512, 14, 14]         --\n","│    └─Conv2d: 2-17                      [16, 512, 14, 14]         2,359,808\n","│    └─ReLU: 2-18                        [16, 512, 14, 14]         --\n","│    └─Conv2d: 2-19                      [16, 512, 14, 14]         2,359,808\n","│    └─ReLU: 2-20                        [16, 512, 14, 14]         --\n","│    └─AvgPool2d: 2-21                   [16, 512, 7, 7]           --\n","├─AdaptiveAvgPool2d: 1-2                 [16, 512, 3, 3]           --\n","├─Linear: 1-3                            [16, 105]                 483,945\n","==========================================================================================\n","Total params: 9,704,425\n","Trainable params: 9,704,425\n","Non-trainable params: 0\n","Total mult-adds (G): 119.89\n","==========================================================================================\n","Input size (MB): 9.63\n","Forward/backward pass size (MB): 950.55\n","Params size (MB): 38.82\n","Estimated Total Size (MB): 999.00\n","=========================================================================================="]},"metadata":{},"execution_count":50}]}],"metadata":{"execution":{"timeout":10800},"colab":{"provenance":[{"file_id":"1YnkPp-Ec265CEhUj-nBwhuzWTSS03XhJ","timestamp":1701804352029}],"collapsed_sections":["GAlsQub2dIDf","dI2bPG2Ddah9"],"machine_shape":"hm","gpuType":"V100"},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}