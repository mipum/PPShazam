{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOJNLj2cuR1T9nLQchtNZw+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## End-to-end test"],"metadata":{"id":"X_fSfJ_Y-z4b"}},{"cell_type":"code","source":["!pip install -q datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E6BZCY2h-2Jm","executionInfo":{"status":"ok","timestamp":1701779643669,"user_tz":-120,"elapsed":7702,"user":{"displayName":"Arie Genkin","userId":"17117459664307926572"}},"outputId":"9be1b4dc-4694-4cb0-f336-f37521951c9a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["import numpy as np\n","import keras\n","from datasets import load_dataset\n","import datasets\n","datasets.logging.set_verbosity_warning()"],"metadata":{"id":"0le6TbmgLZ44"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Choosing the best Cluster Classification *model*"],"metadata":{"id":"j2FPfbqnUL8r"}},{"cell_type":"code","source":["# Setup\n","folder  = '/content/drive/MyDrive/Colab Notebooks/Zama'\n","batch_size = 16\n","enhanced_model = True\n","\n","# load cluster classification models\n","if enhanced_model:\n","    cluster_cls_handles = ['8000_orig__1_siamclusters_enhanced.keras',\n","                           '8000_large_4_siamclusters_enhanced.keras']\n","else:\n","    cluster_cls_handles = ['8000_orig__1_siamclusters',\n","                           '8000_large_4_siamclusters_epoch40']\n","\n","cluster_cls_models = [keras.models.load_model(f'{folder}/{h}') for h in cluster_cls_handles]\n","\n","# load embedding model\n","siamese_embedding_model = keras.models.load_model(f'{folder}/embedding_network_512.h5')     #  USAGE: res = siamese_embedding_model.predict( tf_ds )\n","\n","# Test only for clasters with trained inter-cluster classificaton models\n","for cluster_idx in range(0, 6):\n","\n","    # Test with various augmentation layers\n","    for augm in ['small', 'medium', 'large']:\n","\n","        # Load dataset\n","        ds = load_dataset(f\"arieg/cluster{cluster_idx:02d}_{augm}_10\", split='train')\n","        ds.set_format('tf')\n","\n","        image_test = ds['image'].numpy()\n","        if enhanced_model:\n","            emb_test = siamese_embedding_model.predict( ds.with_format('tf').to_tf_dataset(columns=\"image\", batch_size=batch_size), verbose=0)\n","\n","        print(f\"Dataset cluster{cluster_idx:02d}_{augm}_10\")\n","\n","        # Classify cluster\n","        for i, model in enumerate(cluster_cls_models):\n","            print(f\"    Model {cluster_cls_handles[i]}\")\n","\n","            # Classification inference\n","            if enhanced_model:\n","                res = model.predict([image_test, emb_test], verbose=0)\n","            else:\n","                res = model.predict( image_test, verbose=0)\n","\n","            # Top-1\n","            top1_samples = [sample_idx for sample_idx in range(res.shape[0]) if cluster_idx == np.argmax(res[sample_idx])]\n","            print(f\"        Top-1 {len(top1_samples) / res.shape[0]:.2f}\")\n","\n","            # Top-K calculation\n","            k = 3\n","            top3_samples = [sample_idx for sample_idx in range(res.shape[0]) if cluster_idx in np.argpartition(res[sample_idx], -k)[-k:]]\n","            print(f\"        Top-3 {len(top3_samples) / res.shape[0]:.2f}\")"],"metadata":{"id":"R9dJgKyknLAf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### End-to-End test with the chosen Cluster Classsifcation model\n","Test with datasets of large, medium and small augmentations"],"metadata":{"id":"8vxoN1rPn9Bl"}},{"cell_type":"code","source":["# Setup\n","folder  = '/content/drive/MyDrive/Colab Notebooks/Zama'\n","batch_size = 16\n","\n","enhanced_model = True\n","\n","# load the best cluster classification model\n","cluster_cls_handle = '8000_orig__1_siamclusters_enhanced.keras'\n","                          # '8000_large_4_siamclusters_enhanced.keras'\n","                          # '8000_orig__1_siamclusters'\n","                          # '8000_large_4_siamclusters_epoch40'\n","print(f\" Model {cluster_cls_handle}\")\n","cluster_cls_model = keras.models.load_model(f'{folder}/{cluster_cls_handle}')\n","\n","# load embedding model\n","siamese_embedding_model = keras.models.load_model(f'{folder}/embedding_network_512.h5')     #  USAGE: res = siamese_embedding_model.predict( tf_ds )\n","\n","# Test only for clasters with trained inter-cluster classificaton models\n","for cluster_idx in range(0, 6):\n","\n","    # Create inter-cluster classification model\n","    num_classes = np.load(f\"/content/drive/MyDrive/Colab Notebooks/Zama/top_layers/b_cluster{cluster_idx:02d}_top3.npy\").shape[0]\n","    track_cls_model = create_cls_model(num_classes)\n","\n","    # Load top classification layers' weights\n","    # to make it ready for inference\n","    for i, lyr in enumerate([-4, -2, -1]):\n","\n","        weights = np.load(f\"/content/drive/MyDrive/Colab Notebooks/Zama/top_layers/w_cluster{cluster_idx:02d}_top{i+1}.npy\")\n","        biases = np.load(f\"/content/drive/MyDrive/Colab Notebooks/Zama/top_layers/b_cluster{cluster_idx:02d}_top{i+1}.npy\")\n","\n","        track_cls_model.layers[lyr].set_weights([weights, biases])\n","\n","    # Test with various augmentation layers\n","    for augm in ['small', 'medium', 'large']:\n","\n","        # Load dataset\n","        ds = load_dataset(f\"arieg/cluster{cluster_idx:02d}_{augm}_10\", split='train')\n","        ds.set_format('tf')\n","\n","        image_test = ds['image'].numpy()\n","        label_test = ds['label']\n","        if enhanced_model:\n","            emb_test = siamese_embedding_model.predict( ds.with_format('tf').to_tf_dataset(columns=\"image\", batch_size=batch_size), verbose=0)\n","\n","        print(f\"Dataset cluster{cluster_idx:02d}_{augm}_10\")\n","\n","        # Cluster classification inference\n","        if enhanced_model:\n","            res = cluster_cls_model.predict([image_test, emb_test], verbose=0)\n","        else:\n","            res = cluster_cls_model.predict( image_test, verbose=0)\n","\n","        # Track classification inference\n","        res_track = track_cls_model.predict(image_test, verbose=0)\n","\n","        # Top-1\n","        top1_samples = [sample_idx for sample_idx in range(res.shape[0]) if cluster_idx == np.argmax(res[sample_idx])]\n","        print(f\"        Top-1 cluster  {len(top1_samples) / res.shape[0]:.2f}\")\n","\n","        e2e_good = sum([1 for i in top1_samples if np.argmax(res_track[i]) == int(label_test[i])])\n","        print(f\"        Top-1 end2end  {e2e_good / res.shape[0]:.2f}\")\n","\n","        # Top-K calculation\n","        k = 3\n","        top3_samples = [sample_idx for sample_idx in range(res.shape[0]) if cluster_idx in np.argpartition(res[sample_idx], -k)[-k:]]\n","        print(f\"        Top-3 cluster  {len(top3_samples) / res.shape[0]:.2f}\")\n","\n","        e2e_good = sum([1 for i in top3_samples if np.argmax(res_track[i]) == int(label_test[i])])\n","        print(f\"        Top-3 end2end  {e2e_good / res.shape[0]:.2f}\")"],"metadata":{"id":"32IFW3wGoHto"},"execution_count":null,"outputs":[]}]}